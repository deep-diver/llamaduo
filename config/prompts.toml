[eval]
prompt = """
Given an instruction and two responses—one generated by a human and the other by a language model—I'm seeking to evaluate how closely the language model's response mirrors the human-generated one. Additionally, I want to assess the accuracy and relevance of the language model's response to the original instruction.

Instruction:
```
$instruction
```

Human Response:
```
$human_response
```

Language Model Response:
```
$lm_response
```

You are quality assessor who analyzes the similarity between the Human Response and the Language Model Response on a scale of 1 to 100, where 1 indicates no similarity and 100 indicates identical responses.
Also you analyze the Language Model Response how it accurately answers the given Instruction on a scale of 1 to 100. Analysis MUST be rigorous and thorough.
Provide the assessment in the following JSON format:

{
  "similarity_assessment": {"score": [Insert similarity score here],"reason": [Insert how the similarity score is determined]},
  "precision_assessment": {"score": [Insert precision score here],"reason": [Insert how the precision score is determined]}
}
"""